{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.syntax import Syntax\n",
    "from rich.table import Table\n",
    "\n",
    "def view(item):\n",
    "    console = Console()\n",
    "    with console.pager(styles=True):\n",
    "        console.print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzRAYxwdU8jh"
   },
   "source": [
    "## Specification Formulation\n",
    "\n",
    "After entering the following, press [Run All Cells (Ctrl + F9)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "caSeJzWrUqYM"
   },
   "outputs": [],
   "source": [
    "# @markdown ## Required Fields\n",
    "# @markdown ### Content of the Textbook\n",
    "book_location = 'independence'\n",
    "book_content = \"A textbook on 1971 : Independence of Bangladesh\" #@param {type:\"string\"}\n",
    "# @markdown ### Approximate Number of Pages\n",
    "n_pages = 20 # @param {\"type\":\"integer\",\"placeholder\":\"40\"}\n",
    "# @markdown ### Output Format\n",
    "tex_output = True #@param {type:\"boolean\"}\n",
    "pdf_output = True #@param {type:\"boolean\"}\n",
    "md_output = False #@param {type:\"boolean\"}\n",
    "\n",
    "# @markdown ## Optional Fields\n",
    "# @markdown ### Intended Audience\n",
    "target_readers = \"A fourth-year university student who is already familiar history of Indian subcontinent and has started research on Independence of Bangladesh.\" #@param {type:\"string\"}\n",
    "# @markdown ### Frequency of Equations\n",
    "equation_frequency_level = 1 #@param {type:\"slider\", min:1, max:5, step:1}\n",
    "# @markdown ### Consider Chapter Structure and Organization in Content Generation\n",
    "do_consider_outline = True #@param {type:\"boolean\"}\n",
    "# @markdown ### Consider Previous Output in Content Generation\n",
    "do_consider_previous_sections = True #@param {type:\"boolean\"}\n",
    "# @markdown ### Additional Requirements for Content\n",
    "additional_requirements = \"Try to explain the concept based on factual data.\" #@param {type:\"string\"}\n",
    "\n",
    "if book_content == \"\":\n",
    "    print('\\033[31m'+'Please specify the content of the textbook.'+'\\033[0m')\n",
    "if n_pages == 0:\n",
    "    print('\\033[31m'+'Please specify the number of pages.'+'\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(book_location):\n",
    "    os.makedirs(book_location)\n",
    "os.chdir(book_location)\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spXv6uEbsxis"
   },
   "source": [
    "## Prompt Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6QvuipAzsz3c"
   },
   "outputs": [],
   "source": [
    "# Common Prompt\n",
    "prompt_common = f\"\"\"\n",
    "The following content will be used to write a book.\n",
    "{book_content}\n",
    "The total number of pages in the book is {n_pages}, with an estimated 20 lines per page. Please write in a polite tone.\n",
    "\"\"\"\n",
    "if target_readers != \"\":\n",
    "    prompt_common += f\"The intended readers are as follows:\\n {target_readers}\"\n",
    "if additional_requirements != \"\":\n",
    "    prompt_common += f\"Additionally, please take the following into consideration:\\n {additional_requirements}\"\n",
    "\n",
    "# Prompt for Generating Book and Chapter Titles and Summaries\n",
    "prompt_book_title = prompt_common + \"\"\"\n",
    "Based on the above, please provide the title and summary for the book and each chapter in the following JSON format.\n",
    "The book summary should not only provide an overview but also mention the primary objectives, scope, and depth of the content, in about 5-10 detailed sentences.\n",
    "Please also consider the page allocation for each chapter in units of 0.1, like 0.8 pages.\n",
    "Additionally, consider whether each chapter needs further division based on semantic cohesion (needsSubdivision), and answer true or false.\n",
    "Do not include any speculative or unverified information. Please avoid writing chapter numbers in the titles.\n",
    "The number of sections should vary as needed. \n",
    "Please do not include any extra introductory or conclusory message outside the expected json format.\n",
    "Enclose the json content between ```json and ``` delimiters.\n",
    "\n",
    "The output format should be strictly as follows:\n",
    "\n",
    "```json\n",
    "{{\n",
    "\"title\": \"\",\n",
    "\"summary\": \"\",\n",
    "\"childs\":\n",
    "    [\n",
    "        {{\"title\": \"\", \"summary\": \"\", \"n_pages\": , \"needsSubdivision\": }},\n",
    "        {{\"title\": \"\", \"summary\": \"\", \"n_pages\": , \"needsSubdivision\": }},\n",
    "        {{\"title\": \"\", \"summary\": \"\", \"n_pages\": , \"needsSubdivision\": }}\n",
    "    ]\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for Section Division\n",
    "prompt_section_list_creation = prompt_common + \"\"\"\n",
    "Based on the above information, I plan to create a book titled {book_title}. The book summary is as follows:\n",
    "{book_summary}\n",
    "For the section on {target}, I would like to create content across {n_pages} pages, assuming 40 lines per page.\n",
    "The summary for this section is as follows:\n",
    "{section_summary}\n",
    "Please divide this section into multiple parts and output the titles and summaries of each part in the following JSON format.\n",
    "Also, consider the page allocation for each part in units of 0.1, like 0.8 pages.\n",
    "Additionally, consider whether each part needs further division based on semantic cohesion (needsSubdivision) and answer true or false.\n",
    "Please avoid including section numbers in the titles. \n",
    "Please do not include any extra introductory or conclusory message outside the expected format.\n",
    "Enclose the json content between ```json and ``` delimiters.\n",
    "\n",
    "The output format should be strictly as follows:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {{\"title\": \"\", \"summary\": \"\", \"n_pages\": , \"needsSubdivision\": }},\n",
    "    {{\"title\": \"\", \"summary\": \"\", \"n_pages\": , \"needsSubdivision\": }}\n",
    "]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for Generating Main Content\n",
    "prompt_content_creation = prompt_common + \"\"\"\n",
    "Based on the above, I would like to create a book titled {book_title}. The book summary is as follows:\n",
    "{book_summary}\n",
    "\n",
    "I would like to create content for the section on {target}, across {n_pages} pages, assuming 20 lines per page. \n",
    "The summary for this section is as follows:\n",
    "{section_summary}\n",
    "\n",
    "{toc_and_summary}\n",
    "{previous_sections}\n",
    "\n",
    "Please output the content in the following format:\n",
    "- Enclose the LaTeX content strictly between the delimiters `<|tex_start|>` and `<|tex_end|>`, as shown below:\n",
    "    - The LaTeX content must **start** with `<|tex_start|>`.\n",
    "    - The LaTeX content must **end** with `<|tex_end|>`.\n",
    "- Do not include any other messages, comments, or text outside these delimiters.\n",
    "- For equations, use the `equation` or `align` environment; do not use nested environments. Avoid enclosing `align` blocks within additional brackets (e.g., `\\[ ... \\]`).\n",
    "- For programming code, use the `lstlisting` environment with the appropriate language option (e.g., `[language=Python]`).\n",
    "- Escape all special characters (e.g., `#` as `\\#`, `%` as `\\%`, `_` as `\\_`, `&` as `\\&`, etc.) to ensure valid LaTeX.\n",
    "- Do not include backtick sequences (` ``` `) or any unrelated formatting not part of LaTeX syntax.\n",
    "- Use the `<|tex_start|>` and `<|tex_end|>` delimiters only **once** in the output: at the beginning and end of the LaTeX content.\n",
    "\n",
    "Example output format:\n",
    "    <|tex_start|>\n",
    "    The LaTeX content here, including equations:\n",
    "    \\begin{equation}\n",
    "    E = mc^2\n",
    "    \\end{equation}\n",
    "    And programming code:\n",
    "    \\begin{lstlisting}[language=Python]\n",
    "    print(\"Hello, world!\")\n",
    "    \\end{lstlisting}\n",
    "    <|tex_end|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oV3gAYrto4GT"
   },
   "source": [
    "## Parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TvppBhFjo3EF"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_depth = 2  # If 1, only sections; if 2, includes subsections, and so on.\n",
    "max_output_pages = 1.5  # Maximum number of pages output by the LLM\n",
    "\n",
    "book_node_name = \"book\"  # Name of the root node\n",
    "\n",
    "if do_consider_previous_sections:\n",
    "    n_previous_sections = 1\n",
    "else:\n",
    "    n_previous_sections = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhRrtxVSAxjw"
   },
   "source": [
    "## Library Installation and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3yeQIS2AxOa",
    "outputId": "20ee90ed-03a6-4687-bfcd-2ca8cbf2dfef"
   },
   "outputs": [],
   "source": [
    "#!apt-get update\n",
    "#!apt-get install -y python3-dev graphviz libgraphviz-dev pkg-config\n",
    "#!apt-get install -y latexmk\n",
    "#!apt-get install -y texlive-latex-extra\n",
    "#!apt-get install -y texlive-science\n",
    "#%pip install -qU langchain-openai\n",
    "#%pip install pygraphviz\n",
    "#%pip install pylatex\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import networkx as nx\n",
    "from IPython.display import Markdown\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "from pylatex import Command, Document, Section, Subsection, Package\n",
    "from pylatex.section import Chapter\n",
    "from pylatex.utils import NoEscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5cjjEOefAJR"
   },
   "source": [
    "## Graph Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5x6iAEg2fAR7"
   },
   "outputs": [],
   "source": [
    "book_graph = nx.DiGraph(book_content=book_content, target_readers=target_readers, equation_frequency_level=equation_frequency_level, additional_requirements=additional_requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m{\u001b[0m\n",
      "    \u001b[32m'book_content'\u001b[0m: \u001b[32m'A textbook on 1971 : Independence of Bangladesh'\u001b[0m,\n",
      "    \u001b[32m'target_readers'\u001b[0m: \u001b[32m'A fourth-year university student who is already familiar history of Indian subcontinent and \u001b[0m\n",
      "\u001b[32mhas started research on Independence of Bangladesh.'\u001b[0m,\n",
      "    \u001b[32m'equation_frequency_level'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
      "    \u001b[32m'additional_requirements'\u001b[0m: \u001b[32m'Try to explain the concept based on factual data.'\u001b[0m\n",
      "\u001b[1m}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view(book_graph.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muWhg5hpTLId"
   },
   "source": [
    "## Title and Chapter Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7i5EKOOkBVG"
   },
   "source": [
    "### Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tbrOoC1oj_Ou"
   },
   "outputs": [],
   "source": [
    "def extract_book_and_chapter_contents(markdown_text):\n",
    "    \"\"\"\n",
    "    Function that extracts the first JSON data found in Markdown text\n",
    "    and converts it to a Python dictionary.\n",
    "\n",
    "    Args:\n",
    "        markdown_text (str): A string in Markdown format, assumed to contain\n",
    "        JSON-formatted data within it.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: Returns a dictionary if a valid JSON is found;\n",
    "        returns None if no JSON is found or parsing fails.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the starting point of JSON in the Markdown\n",
    "    start_index = markdown_text.find('{')\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "\n",
    "    # Traverse the entire string and check the balance of nested braces\n",
    "    brace_count = 0\n",
    "    for i in range(start_index, len(markdown_text)):\n",
    "        if markdown_text[i] == '{':\n",
    "            brace_count += 1\n",
    "        elif markdown_text[i] == '}':\n",
    "            brace_count -= 1\n",
    "\n",
    "        # Extract when braces are balanced\n",
    "        if brace_count == 0:\n",
    "            json_string = markdown_text[start_index:i+1]\n",
    "            try:\n",
    "                # Convert to JSON format\n",
    "                json_data = json.loads(json_string)\n",
    "                view(json_data)\n",
    "                return json_data\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON parsing error: {e}\")\n",
    "                return None\n",
    "            return\n",
    "\n",
    "    # If no closing brace is found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISN0KhB-kAuo"
   },
   "source": [
    "### Output by LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptwpNLJNTDuf"
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = OllamaLLM(base_url='http://localhost:11434', model=\"llama3.2:latest\")\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_book_title)\n",
    "\n",
    "chain = prompt | llm\n",
    "result = chain.invoke(\n",
    "    {\n",
    "        \"book_content\": book_content,\n",
    "        \"target_readers\": target_readers,\n",
    "        \"n_pages\": n_pages,\n",
    "        \"additional_requirements\": additional_requirements\n",
    "   }\n",
    ")\n",
    "\n",
    "book_json = extract_book_and_chapter_contents(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhgOs-a4kJ59"
   },
   "source": [
    "### Storing Results in the Book Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdrOGJfqBcf-"
   },
   "outputs": [],
   "source": [
    "# About the book\n",
    "book_graph.add_nodes_from([(book_node_name, {\"title\": book_json[\"title\"], \"summary\": book_json[\"summary\"], \"n_pages\": n_pages, \"needsSubdivision\": True})])\n",
    "\n",
    "# About chapters (sections)\n",
    "book_graph.add_nodes_from([(str(idx+1), child) for idx, child in enumerate(book_json[\"childs\"])])\n",
    "book_graph.add_edges_from([(book_node_name, str(idx+1)) for idx in range(len(book_json[\"childs\"]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wc84vrut-pZ9"
   },
   "source": [
    "## Title and Structure Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WW4r36Pr-_Po"
   },
   "source": [
    "### Displaying Title and Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 926
    },
    "id": "vzGKlqImWtl4",
    "outputId": "c4c26c3a-a0cd-47da-bbd0-e836af5be9bf"
   },
   "outputs": [],
   "source": [
    "book_node = book_graph.nodes[book_node_name]\n",
    "\n",
    "content_md = \"\"\n",
    "content_md += \"\\n ## Title: \" + book_node[\"title\"] + \" (Page Count: \" + str(book_node[\"n_pages\"]) + \")\"\n",
    "content_md += \"\\n \" + book_node[\"summary\"]\n",
    "for idx, child_node_name in enumerate(book_graph.successors(book_node_name)):\n",
    "    child_node = book_graph.nodes[child_node_name]\n",
    "    content_md += \"\\n ### Chapter \" + str(idx+1) + \": \" + child_node[\"title\"] + \" (Page Count: \" + str(child_node[\"n_pages\"]) + \")\"\n",
    "    content_md += \"\\n\" + child_node[\"summary\"]\n",
    "\n",
    "Markdown(content_md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfXBblAq_mx6"
   },
   "source": [
    "## Book Graph Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaiIwjSjknP5"
   },
   "source": [
    "### Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgA7TIry_W2D"
   },
   "outputs": [],
   "source": [
    "def extract_section_list(markdown_text):\n",
    "\n",
    "    pattern = r'```json\\s*(.*?)\\s*```'\n",
    "    match = re.search(pattern, markdown_text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        json_string = match.group(1)\n",
    "        data = json.loads(json_string)\n",
    "        return data\n",
    "    else:\n",
    "        print(\"JSON data not found.\")\n",
    "        return None\n",
    "\n",
    "def extract_section_content(markdown_text):\n",
    "    pattern = r'<\\|tex_start\\|>\\s*(.*?)\\s*<\\|tex_end\\|>'  # Regex to match custom delimiters\n",
    "    match = re.search(pattern, markdown_text, re.DOTALL)  # Search across lines (re.DOTALL)\n",
    "    if match:\n",
    "        tex_string = match.group(1)  # Extract content between the delimiters\n",
    "        return tex_string\n",
    "    else:\n",
    "        pattern = r'<\\|tex_start\\|>\\s*(.*?)\\s*<\\/\\|tex_end\\|>'  # Regex to match custom delimiters\n",
    "        match = re.search(pattern, markdown_text, re.DOTALL)  # Search across lines (re.DOTALL)\n",
    "        if match:\n",
    "            tex_string = match.group(1)  # Extract content between the delimiters\n",
    "            return tex_string\n",
    "        else:\n",
    "            print(\"TeX data not found.\")  # Handle cases with missing delimiters\n",
    "            return markdown_text\n",
    "\n",
    "\n",
    "def get_equation_frequency(equation_frequency_level):\n",
    "    if equation_frequency_level == 1:\n",
    "        return \"Avoid using equations whenever possible, and explain all concepts in simple words. Use equations only when absolutely necessary and keep it to a minimum.\"\n",
    "    elif equation_frequency_level == 2:\n",
    "        return \"Use equations sparingly, focusing primarily on explanations in prose. Use simple equations only if necessary.\"\n",
    "    elif equation_frequency_level == 3:\n",
    "        return \"Combine equations and prose explanations in a balanced way. Use equations to illustrate key concepts, supplementing with prose where needed.\"\n",
    "    elif equation_frequency_level == 4:\n",
    "        return \"Use equations actively to precisely convey concepts and relationships. However, important explanations should also be supplemented with prose.\"\n",
    "    elif equation_frequency_level == 5:\n",
    "        return \"Use equations extensively. Express as many concepts and relationships as possible through equations.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h67SUYLNkfjh"
   },
   "source": [
    "### LLMによる章立ての出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvZSOTxb4kpT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "book_node = book_graph.nodes[book_node_name]\n",
    "next_parent_list = [book_node_name]\n",
    "\n",
    "for depth in range(max_depth):\n",
    "    parent_list = next_parent_list\n",
    "    next_parent_list = []\n",
    "    for parent_node_name in parent_list:\n",
    "        for _, child_node_name in enumerate(book_graph.successors(parent_node_name)):\n",
    "            parant_node = book_graph.nodes[parent_node_name]\n",
    "            child_node = book_graph.nodes[child_node_name]\n",
    "\n",
    "            view(child_node)\n",
    "\n",
    "            if (child_node[\"needsSubdivision\"] or float(child_node[\"n_pages\"]) >= max_output_pages) and depth < max_depth-1:\n",
    "\n",
    "                # Output by LLM\n",
    "                prompt = PromptTemplate.from_template(prompt_section_list_creation)\n",
    "                chain = prompt | llm\n",
    "\n",
    "                result = chain.invoke(\n",
    "                    {\n",
    "                        \"book_title\": book_node[\"title\"],\n",
    "                        \"book_summary\": book_node[\"summary\"],\n",
    "                        \"equation_frequency\": get_equation_frequency(book_graph.graph[\"equation_frequency_level\"]),\n",
    "                        \"target\": child_node[\"title\"],\n",
    "                        \"n_pages\": child_node[\"n_pages\"],\n",
    "                        \"section_summary\": child_node[\"summary\"]\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Convert output to dictionary format\n",
    "                section_json = extract_section_list(result)\n",
    "                view(section_json)\n",
    "\n",
    "                # Create graph nodes and store results\n",
    "                book_graph.add_nodes_from([(child_node_name + \"-\" + str(idx+1), grandchild) for idx, grandchild in enumerate(section_json)])\n",
    "                book_graph.add_edges_from([(child_node_name, child_node_name + \"-\" + str(idx+1)) for idx in range(len(section_json))])\n",
    "\n",
    "                # Only set as the next parent if subdivided\n",
    "                next_parent_list.append(child_node_name)\n",
    "\n",
    "            elif depth == (max_depth-1) or (child_node and not child_node[\"needsSubdivision\"]):\n",
    "\n",
    "                # Create graph nodes and store results\n",
    "                book_graph.add_nodes_from([(child_node_name + \"-p\", {\"content_file_path\": child_node_name + \"-p.tex\"})])\n",
    "                book_graph.add_edges_from([(child_node_name, child_node_name + \"-p\")])\n",
    "\n",
    "            else:\n",
    "                print(\"Error: needsSubdivision attribute is not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp4RAdzFe-LT"
   },
   "source": [
    "### 本文の内容の出力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaDh7M9IUY-j"
   },
   "source": [
    "#### 関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5V5j0SWUb_G"
   },
   "outputs": [],
   "source": [
    "def extract_content_list(string_list):\n",
    "    \"\"\"\n",
    "    This function extracts only the strings that match a specific pattern\n",
    "    (a combination of numbers and hyphens ending in '-p') from the input string_list\n",
    "    and returns them as a new list.\n",
    "    \"\"\"\n",
    "    pattern = r'(?:\\d+-)*\\d+-p'\n",
    "    return [s for s in string_list if re.match(pattern, s)]\n",
    "\n",
    "def custom_sort_key(s):\n",
    "    \"\"\"\n",
    "    This function splits the string s by the numerical parts,\n",
    "    converting them into a list of integers,\n",
    "    thereby generating a custom key for numerical sorting.\n",
    "    \"\"\"\n",
    "    parts = re.split(r'[-p]', s)\n",
    "    return [int(part) for part in parts if part != '']\n",
    "\n",
    "def sort_strings(string_list):\n",
    "    \"\"\"\n",
    "    This function sorts the input string_list using the custom key\n",
    "    defined by the custom_sort_key function and returns the sorted list.\n",
    "    \"\"\"\n",
    "    sorted_strings = sorted(string_list, key=custom_sort_key)\n",
    "    return sorted_strings\n",
    "\n",
    "def sort_leaf_nodes(book_graph):\n",
    "    \"\"\"\n",
    "    This function extracts the leaf nodes of book_graph based on a specific pattern\n",
    "    and returns a list sorted in numerical order.\n",
    "\n",
    "    Arguments:\n",
    "    - book_graph: A graph object with nodes labeled by strings\n",
    "\n",
    "    Returns:\n",
    "    - sorted_content_str_list: A sorted list of strings\n",
    "    \"\"\"\n",
    "    # Extract strings matching a specific pattern from the nodes of book_graph\n",
    "    content_str_list = extract_content_list(list(book_graph.nodes))\n",
    "\n",
    "    # Sort the extracted list of strings using a custom key\n",
    "    sorted_content_str_list = sort_strings(content_str_list)\n",
    "\n",
    "    return sorted_content_str_list\n",
    "\n",
    "def generate_outline(book_graph, book_node_name):\n",
    "    \"\"\"\n",
    "    A function to output the chapter structure to be considered for the main text output.\n",
    "    \"\"\"\n",
    "\n",
    "    toc_and_summary = \"The chapter structure is shown below.\\n\"\n",
    "\n",
    "    # Sort the leaf nodes with content in order\n",
    "    sorted_content_str_list = sort_leaf_nodes(book_graph)\n",
    "\n",
    "    # Create the table of contents\n",
    "    toc_and_summary += \"# \" + book_graph.nodes[book_node_name][\"title\"] + \"\\n\"\n",
    "    toc_and_summary += book_graph.nodes[book_node_name][\"summary\"] + \"\\n\"\n",
    "\n",
    "    for heading_number_str in sorted_content_str_list:\n",
    "        heading_number = custom_sort_key(heading_number_str)\n",
    "\n",
    "        # Chapter\n",
    "        if len(heading_number[1:]) == 0 or all(x == 1 for x in heading_number[1:]):\n",
    "            node_name = \"-\".join(map(str, heading_number[0:1]))\n",
    "            toc_and_summary += \"## \" + book_graph.nodes[node_name][\"title\"] + \"\\n\"\n",
    "            toc_and_summary += book_graph.nodes[node_name][\"summary\"] + \"\\n\"\n",
    "\n",
    "        # Section\n",
    "        if (len(heading_number[2:]) == 0 and len(heading_number[:2]) > 1) or (len(heading_number[2:]) > 0 and all(x == 1 for x in heading_number[2:])):\n",
    "            node_name = \"-\".join(map(str, heading_number[0:2]))\n",
    "            toc_and_summary += \"### \" + book_graph.nodes[node_name][\"title\"] + \"\\n\"\n",
    "            toc_and_summary += book_graph.nodes[node_name][\"summary\"] + \"\\n\"\n",
    "\n",
    "        # Subsection\n",
    "        if (len(heading_number[3:]) == 0 and len(heading_number[:3]) > 2) or (len(heading_number[3:]) > 0 and all(x == 1 for x in heading_number[3:])):\n",
    "            node_name = \"-\".join(map(str, heading_number[0:3]))\n",
    "            toc_and_summary += \"#### \" + book_graph.nodes[node_name][\"title\"] + \"\\n\"\n",
    "            toc_and_summary += book_graph.nodes[node_name][\"summary\"] + \"\\n\"\n",
    "\n",
    "    return toc_and_summary\n",
    "\n",
    "def slide_list_with_new_title_and_content(title, content, original_list):\n",
    "    # Create a shifted list, adding a new element (a dictionary with title and content) at the 0th position\n",
    "    slid_list = [{\"title\": title, \"content\": content}] + original_list[:-1]\n",
    "    return slid_list\n",
    "\n",
    "def generate_prompt_for_previous_sections(previous_sections_content_list, n_previous_sections):\n",
    "    \"\"\"\n",
    "    This function generates a prompt based on previous sections.\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "\n",
    "    for i in range(n_previous_sections):\n",
    "        i_previous_section = previous_sections_content_list[i]\n",
    "        title = i_previous_section[\"title\"]\n",
    "        content = i_previous_section[\"content\"]\n",
    "        prompt += f\"The title of part {i+1} sections ago is {title}, and the content is as follows.\\n{content}\\n\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "panRk8LBUdHL"
   },
   "source": [
    "#### 本文出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YX1GXy4te-Z2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Output of chapter structure to consider when generating content\n",
    "toc_and_summary = \"\"\n",
    "if do_consider_outline:\n",
    "    toc_and_summary = generate_outline(book_graph, book_node_name)\n",
    "\n",
    "# Initialize a list of strings to store previous outputs\n",
    "previous_sections_content_list = [{\"title\": \"\", \"content\": \"\"} for i in range(n_previous_sections)]\n",
    "\n",
    "# Sort the nodes containing the main content (leaf nodes) in order\n",
    "sorted_content_str_list = sort_leaf_nodes(book_graph)\n",
    "\n",
    "# Append the main content\n",
    "for heading_number_str in sorted_content_str_list:\n",
    "    heading_number = custom_sort_key(heading_number_str)\n",
    "\n",
    "    node_name = \"-\".join(map(str, heading_number))\n",
    "    node = book_graph.nodes[node_name]\n",
    "\n",
    "    # Generate output via LLM\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(prompt_content_creation)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    result = chain.invoke(\n",
    "        {\n",
    "            \"book_title\": book_node[\"title\"],\n",
    "            \"book_summary\": book_node[\"summary\"],\n",
    "            \"toc_and_summary\": toc_and_summary,\n",
    "            \"previous_sections\": generate_prompt_for_previous_sections(previous_sections_content_list, n_previous_sections),\n",
    "            \"equation_frequency\": get_equation_frequency(book_graph.graph[\"equation_frequency_level\"]),\n",
    "            \"target\": node[\"title\"],\n",
    "            \"n_pages\": node[\"n_pages\"],\n",
    "            \"section_summary\": node[\"summary\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    view(result)\n",
    "    contents_tex = extract_section_content(result)\n",
    "    with open(node_name + \"-p.tex\", mode='w', encoding='UTF-8') as f:\n",
    "        f.write(contents_tex)\n",
    "\n",
    "    # Retain past outputs\n",
    "    if do_consider_previous_sections:\n",
    "        previous_sections_content_list = slide_list_with_new_title_and_content(node[\"title\"], contents_tex, previous_sections_content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2LSx8U7PgLB"
   },
   "source": [
    "## 本グラフの表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 783
    },
    "id": "zslTqJiALTtk",
    "outputId": "25eceaf9-73af-4a7f-80a5-909543a81e20"
   },
   "outputs": [],
   "source": [
    "pos = graphviz_layout(book_graph, prog=\"dot\")\n",
    "\n",
    "# matplotlib settings\n",
    "fig = plt.figure(figsize=(20, 10), dpi=300)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# Color the leaf nodes (content nodes) in red\n",
    "node_color_list = [\"r\" if \"p\" in s else \"w\" for s in list(book_graph.nodes)]\n",
    "\n",
    "# draw the network\n",
    "nx.draw(book_graph,\n",
    "        ax=ax,\n",
    "        pos=pos,\n",
    "        with_labels=True,\n",
    "        node_size=300,\n",
    "        node_color=node_color_list,\n",
    "        alpha=0.3,\n",
    "        node_shape='.',\n",
    "        width=0.5)\n",
    "\n",
    "print(\"The arrows starting from 'book' represent chapters, with sections below them and subsections below the sections.\\nThese nodes contain information such as the title and summary of each chapter or section.\\nAdditionally, the end nodes marked in red circles hold information on the content of the text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo3GFevWPjpy"
   },
   "source": [
    "## 本の作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Of2bOA5hPs0Q"
   },
   "source": [
    "### latexmkrcファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYXVeE97MhsD",
    "outputId": "9f6229a9-27f6-403b-859d-1f4c4a010be0"
   },
   "outputs": [],
   "source": [
    "# Get the path of the home directory\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "\n",
    "# Create the path for the .latexmkrc file\n",
    "latexmkrc_path = os.path.join(home_dir, \".latexmkrc\")\n",
    "\n",
    "# Contents of the latexmkrc file\n",
    "content = '''$latex = 'pdflatex -synctex=1 -halt-on-error -interaction=nonstopmode %O %S';\n",
    "$bibtex = 'bibtex %O %S';\n",
    "$biber = 'biber %O %S';\n",
    "$makeindex = 'makeindex %O -o %D %S';\n",
    "$dvipdf = 'dvipdfmx %O -o %D %S';\n",
    "\n",
    "$max_repeat = 5;\n",
    "$pdf_mode = 1;'''\n",
    "\n",
    "# Create the file and write the content\n",
    "try:\n",
    "    with open(latexmkrc_path, \"w\") as file:\n",
    "        file.write(content)\n",
    "    print(f\".latexmkrc file has been created successfully at {latexmkrc_path}\")\n",
    "except IOError as e:\n",
    "    print(f\"An error occurred while creating the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJzQSz1qPxtj"
   },
   "source": [
    "### LaTeXで本文の作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzDdxuC9lwXQ"
   },
   "source": [
    "### LaTeXドキュメントの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtEA6rVWPv1R"
   },
   "outputs": [],
   "source": [
    "# Creating a PDF using pylatex\n",
    "geometry_options = {\"tmargin\": \"3cm\", \"lmargin\": \"3cm\"}\n",
    "doc = Document(documentclass=\"report\", geometry_options=geometry_options)\n",
    "\n",
    "# Adding preamble and title\n",
    "doc.packages.append(Package('amsmath'))\n",
    "doc.packages.append(Package('amssymb'))\n",
    "doc.packages.append(Package('amsfonts'))\n",
    "doc.packages.append(Package('mathtools'))\n",
    "doc.packages.append(Package('bm'))\n",
    "doc.packages.append(Package('physics'))\n",
    "doc.packages.append(Package('inputenc', options=\"utf8\"))\n",
    "doc.packages.append(Package('listings'))\n",
    "doc.packages.append(Package('jvlisting'))\n",
    "doc.packages.append(Package('color'))\n",
    "doc.packages.append(Package('underscore', options=\"strings\"))\n",
    "doc.preamble.append(Command(\"title\", book_graph.nodes[book_node_name][\"title\"]))\n",
    "doc.preamble.append(Command(\"date\", NoEscape(r\"\\today\")))\n",
    "doc.append(NoEscape(r\"\\maketitle\"))\n",
    "doc.append(NoEscape(r\"\\tableofcontents\"))\n",
    "doc.append(NoEscape(r'\\lstset{ backgroundcolor={\\color[gray]{.90}}, breaklines = true, breakindent = 10pt, basicstyle = \\ttfamily\\scriptsize, commentstyle = {\\itshape \\color[cmyk]{1,0.4,1,0}}, classoffset = 0, keywordstyle = {\\bfseries \\color[cmyk]{0,1,0,0}}, stringstyle = {\\ttfamily \\color[rgb]{0,0,1}}, frame = TBrl, framesep = 5pt, numbers = left, stepnumber = 1, numberstyle = \\tiny, tabsize = 4, captionpos = t}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yN9GMsGGRX3i"
   },
   "outputs": [],
   "source": [
    "# Arrange nodes containing the main text content in order\n",
    "content_str_list = extract_content_list(list(book_graph.nodes))\n",
    "sorted_content_str_list = sort_strings(content_str_list)\n",
    "\n",
    "# Add main content\n",
    "for heading_number_str in sorted_content_str_list:\n",
    "    heading_number = custom_sort_key(heading_number_str)\n",
    "\n",
    "    # Add chapter title\n",
    "    if len(heading_number[1:]) == 0 or all(x == 1 for x in heading_number[1:]):\n",
    "        node_name = \"-\".join(map(str, heading_number[0:1]))\n",
    "        with doc.create(Chapter(book_graph.nodes[node_name][\"title\"], label=False)):\n",
    "            doc.append(NoEscape(book_graph.nodes[node_name][\"summary\"].replace(\"\\\\\\\\\",\"\\\\\")))\n",
    "\n",
    "    # Add section title\n",
    "    if (len(heading_number[2:]) == 0 and len(heading_number[:2]) > 1) or (len(heading_number[2:]) > 0 and all(x == 1 for x in heading_number[2:])):\n",
    "        node_name = \"-\".join(map(str, heading_number[0:2]))\n",
    "        with doc.create(Section(book_graph.nodes[node_name][\"title\"], label=False)):\n",
    "            doc.append(NoEscape(book_graph.nodes[node_name][\"summary\"].replace(\"\\\\\\\\\",\"\\\\\")))\n",
    "\n",
    "    # Add subsection title\n",
    "    if (len(heading_number[3:]) == 0 and len(heading_number[:3]) > 2) or (len(heading_number[3:]) > 0 and all(x == 1 for x in heading_number[3:])):\n",
    "        node_name = \"-\".join(map(str, heading_number[0:3]))\n",
    "        with doc.create(Subsection(book_graph.nodes[node_name][\"title\"], label=False)):\n",
    "            doc.append(NoEscape(book_graph.nodes[node_name][\"summary\"].replace(\"\\\\\\\\\",\"\\\\\")))\n",
    "\n",
    "    # Add main text content\n",
    "    tex_file_path = book_graph.nodes[heading_number_str][\"content_file_path\"]\n",
    "    try:\n",
    "        with open(tex_file_path, \"r\", encoding='UTF-8') as file:\n",
    "            tex_content = file.read()\n",
    "            doc.append(NoEscape(tex_content))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58jBeL42P2SF"
   },
   "source": [
    "### PDFファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DclfbWmyP2d2",
    "outputId": "8312337f-855c-4e70-ea10-958136f0e04b"
   },
   "outputs": [],
   "source": [
    "# Compilation. A file named /content/textbook.pdf will be created\n",
    "n_displayed_line = 10\n",
    "isErrorOccured = False\n",
    "\n",
    "try:\n",
    "    doc.generate_pdf(book_node[\"title\"], compiler=\"latexmk\", clean_tex=False)\n",
    "except:\n",
    "    log_file_path = book_node[\"title\"] + \".log\"\n",
    "\n",
    "    # Read the contents of the log file\n",
    "    with open(log_file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Define error output pattern (file-line-error option format)\n",
    "    pattern = r'([a-zA-Z0-9_.]+):(\\d+): (.+)'\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            isErrorOccured = True\n",
    "            print(f\"Line: {match.group(2)}\")\n",
    "            print(f\"Message: \\n  {match.group(3)}\")\n",
    "            for j in range(i+1, min(i+n_displayed_line, len(lines))):\n",
    "                print(lines[j].strip())\n",
    "            print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xam6V7GySM4p"
   },
   "source": [
    "### Markdownファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HO3zD-07Bn1_"
   },
   "outputs": [],
   "source": [
    "def clean_markdown_content(content):\n",
    "    # Remove the part before the first heading (LaTeX settings section)\n",
    "    content = content.split('#', 1)[1]\n",
    "\n",
    "    # Remove % only when it has whitespace before or after\n",
    "    content = re.sub(r\"(?<=\\s)%|%(?=\\s)\", \"\", content)\n",
    "\n",
    "    # Remove % only when it appears at the end of a line\n",
    "    content = re.sub(r\"%\\s*$\", \"\", content)\n",
    "\n",
    "    # Replace multiple newlines with a single newline\n",
    "    content = re.sub(r\"\\n{2,}\", \"\\n\\n\", content)\n",
    "\n",
    "    return content\n",
    "\n",
    "def convert_to_latex_to_katex(content):\n",
    "    # Convert align and align* environments\n",
    "    content = re.sub(r'\\\\begin{align\\*?}', r'$$\\n\\\\begin{aligned}', content)\n",
    "    content = re.sub(r'\\\\end{align\\*?}', r'\\\\end{aligned}\\n$$', content)\n",
    "\n",
    "    # Convert equation and equation* environments\n",
    "    content = re.sub(r'\\\\begin{equation\\*?}', r'$$', content)\n",
    "    content = re.sub(r'\\\\end{equation\\*?}', r'$$', content)\n",
    "\n",
    "    return content\n",
    "\n",
    "def transform_code_blocks(content):\n",
    "\n",
    "    # Extract and handle language option\n",
    "    def replace_lstlisting(match):\n",
    "        options = match.group(1)\n",
    "        code = match.group(2)\n",
    "        # Search for language option\n",
    "        lang_match = re.search(r'language=([a-zA-Z]+)', options)\n",
    "        lang = lang_match.group(1) if lang_match else ''\n",
    "        # Convert to Markdown format\n",
    "        return f'```{lang}\\n{code}\\n```'\n",
    "\n",
    "    # Conversion process for lstlisting\n",
    "    content = re.sub(r'\\\\begin{lstlisting}\\[(.*?)\\](.*?)\\\\end{lstlisting}', replace_lstlisting, content, flags=re.DOTALL)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MXiU2T1gSPpf",
    "outputId": "de628cbf-da5f-417c-8b0d-8977e577e19e"
   },
   "outputs": [],
   "source": [
    "# if md_output:\n",
    "#     %pip install latex2markdown\n",
    "#     import latex2markdown\n",
    "\n",
    "#     with open(book_node[\"title\"] + \".tex\", \"r\") as f:\n",
    "#         latex_string = f.read()\n",
    "\n",
    "#     # Convert LaTeX to Markdown\n",
    "#     processed_latex_str = transform_code_blocks(latex_string)\n",
    "#     l2m = latex2markdown.LaTeX2Markdown(processed_latex_str)\n",
    "#     markdown_string = l2m.to_markdown()\n",
    "\n",
    "#     # Format the Markdown text\n",
    "#     cleaned_md_str = clean_markdown_content(markdown_string)\n",
    "#     katex_md_str = convert_to_latex_to_katex(cleaned_md_str)\n",
    "\n",
    "#     with open(book_node[\"title\"] + \".md\", \"w\") as f:\n",
    "#         f.write(katex_md_str)\n",
    "#     files.download(book_node[\"title\"] + \".md\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
